---
layout:     post
title:      hive环境的搭建及运行
subtitle:   
date:       2019-2-2
author:     BY KiloMeter
header-img: img/2019-2-2-hive环境的搭建及运行/16.jpg
catalog: true
tags:
    - 大数据
    - Hive
---

hive可以说是MapReduce的客户端，因为是客户端，所以无需在每台机器上都安装，只需要在一台机器上安装即可。由于hive的配置需要用到mysql，因此先要在本机安装好mysql。

### mysql的安装

```shell
yum -y install mysql mysql-server mysql-devel

wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm

rpm -ivh mysql-community-release-el7-5.noarch.rpm

yum -y install mysql-community-server
```

安装后 使用service mysqld start启动mysql服务,使用mysql -u root -p启动数据库，等待输入密码时，由于是第一次安装，直接回车即可。



### mysql配置

```shell
systemctl start mysqld.service
```

上面配置后，以后mysql服务会随电脑的启动而自动开启。

```shell
mysqladmin -uroot password '*****'
```

设置mysql的root用户密码。

```shell
grant all on *.* to root@'niubike3' identified by '*******';
```

grant :授权

all：所有权限

\*.\*:数据库名.表名称

root：操作mysql的用户

@'....'：授权给的机器

’\*\*\*\*\*\*‘：mysql密码

### hive的安装与配置

1、hive包的解压

2、修改{hive解压路径}/conf/hive-default.xml.template和{hive解压路径}/conf/hive-env.sh.template

首先复制这两个文件并重命名为hive-site.xml和hive-env.sh。

hive-env.sh的修改：

添加HADOOP_HOME和JAVA_HOME

```shell
HADOOP_HOME=/usr/local/software/hadoop-2.5.0-cdh5.3.6
JAVA_HOME=/usr/local/software/jdk1.8.0_191
```

添加HIVE启动时加载的配置文件位置

```shell
HIVE_CONF_DIR=/usr/local/software/hive/conf
```

修改hive-site.xml文件

该文件里面的内容很多，因此可以先把该文件拉到windows下进行修改后，再上传至linux。也可以使用sublime，安装sftp插件后，直接连上配置文件进行修改。

1、修改

```xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <!--修改成jdbc，以后hive的元数据存储到mysql-->
  <value>jdbc:mysql://niubike1:3306/metastore?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>root</value>
  <description>username to use against metastore database</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>313976009</value>
  <description>password to use against metastore database</description>
</property>
```

3、修改conf的log4j文件

首先先把hive-log4j.properties.template配置文件复制一份并且改名为hive-log4j.properties

```xml
hive.log.dir=/usr/local/software/hive/logs
```

修改日志位置。

### 拷贝数据库驱动包

由于hive使用需要用到mysql，需要在hive的lib文件夹中引入mysql的驱动包

解压mysql-connector-java-5.1.27.tar.gz，进入解压后的文件夹，把里面的mysql-connector-java-5.1.27-bin.jar

复制到{hive的文件夹路径}/lib下

### hive启动

在hive的目录下 使用bin/hive启动

![](/img/2019-2-2-hive环境的搭建及运行/hive启动文件匹配缺失报错.png)

如果出现该错误，去hive-site.xml对应行数查看以下，我这边发现少了一个\<property\>

![](/img/2019-2-2-hive环境的搭建及运行/hive启动mysql缺失授权报错.png)

出现该错误，是数据库没有给上面用户机器进行授权，查看上面使用的grant语句，对报错的用户进行授权。

成功进入后

创建数据库报错

![](/img/2019-2-2-hive环境的搭建及运行/没有开启hadoop报错.png)

问题在于没有开启hadoop，开启即可。

![](/img/2019-2-2-hive环境的搭建及运行/hive成功运行.png)

执行建库，使用数据库等操作没问题，如上图，即为成功。

### hive的运行

从我们上面的配置可以看出来，hive的元数据是存储在mysql中的，我们可以切换下窗口，查看一下mysql，可以看到mysql多了一个数据库metastore，里面存放的是hive的元数据信息。如果删掉这个数据库，那么hive将无法使用，所以这个数据库十分重要，需要定期进行备份。



### hive的一些其他配置

```xml
<property>
  <name>hive.cli.print.header</name>
  <value>true</value>
  <description>Whether to print the names of the columns in query output.</description>
</property>

<property>
  <name>hive.cli.print.current.db</name>
  <value>true</value>
  <description>Whether to include the current database in the Hive prompt.</description>
</property>
```

第一个配置改为true后，以后在hive的查询中，会显示查询出来的字段名称

第二个配置为true后，会显示出当前正在使用的数据库

![](/img/2019-2-2-hive环境的搭建及运行/显示当前数据库名称.png)

```xml
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/user/hive/warehouse</value>
  <description>location of default database for the warehouse</description>
</property>
```

在hive中插入数据后，数据会保存到上述配置的路径中，注意，该路径是hdfs目录路径，而不是本地的路径。

### hive的一些基本语句

#### 创建数据库

create database 数据库名;

#### 创建表

- 创建空表：

  create table t1(eid int,name string,sex string) row format delimited fields terminated by '\t';

  row format delimited fields terminated by '\*\*\*';

  最后这个语句表示输入的数据(一般数据是来源于hdfs)用'\*\*\*'符号进行分割

- 复制其他表结构和数据

  create table t1_tmp as select  \*(这里添加想要复制的字段) from 要复制的表名;

- 复制其他表结构

  create table t1_tmp like 要复制的表名;

  

#### 查询语句：select \* from t1;

查看表的详细信息：desc formatted 表名

#### 数据导入

- 本地导入

  load data **local**  inpath '文件路径' into table 表名;

  ![](/img/2019-2-2-hive环境的搭建及运行/导入本地数据到hive.png)

- HDFS导入

  load data  inpath '文件路径' into table 表名;

- 覆盖导入

  load data local inpath '文件路径' overwrite into table 表名;

  load data inpath '文件路径' overwrite into table 表名;

- 查询导入

  create table 表名 as select \* from  要插入的表名;

- insert导入

  - 追加-append-默认方式

    insert into table 表名  select \* from 要插入的表名;

  - 覆盖-overwrite-显示指定-使用频率高

    insert overwrite table 表名  select \* from 要插入的表名;

  

  

  