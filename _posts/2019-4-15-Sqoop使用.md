---
layout:     post
title:      Sqoop使用
subtitle:   
date:       2019-4-15
author:     BY KiloMeter
header-img: img/2019-4-15-Sqoop使用/39.png
catalog: true
tags:
    - Sqoop
---
Sqoop是个工具，方便让关系型数据库，HDFS和hive之间的数据进行轻松的转换。

Sqoop的安装比较简单，这里就不展示了，这里主要展示Sqoop的使用

Sqoop使用的参数有下面14个，使用Sqoop help可以查看

![](/img/2019-4-15-Sqoop使用/Sqoop可用参数.PNG)

```shell
# list-tables参数是查看选中数据库中的所有表
bin/sqoop list-tables --connect jdbc:mysql://localhost:3306/数据库名 --username 用户名 --password 密码
# list-databases参数查看所有数据库
bin/sqoop list-databases --connect jdbc:mysql://localhost:3306/数据库名 --username 用户名 --password 密码
# create-hive-table参数是在hive上创建一张和关系型数据库中一样表结构的表 如果hive上已经有了的话则创建失败（注：仅是创建表结构，并没有导入数据）
bin/sqoop create-hive-table --connect jdbc:mysql://localhost:3306/company --username 用户名 --password 密码 --table staff --hive-table testStaff
```



```shell
# import命令是将关系型数据库的数据导入到hdfs或hive
# --hive-table参数指定把数据导入到hive中   -m参数指定mapreduce个数
# 注：这里使用localhost会报错，得改用成IP，还有，导入数据到hive时，数据先是会存放到hdfs上的/user/用户名/表名
# 目录下，如果该目录已经存在了，执行过程也会报错
# 该命令是把数据追加到原有的表中
bin/sqoop import --connect jdbc:mysql://主机IP:3306/company --username 用户名  --password 密码 --table staff --hive-table teststaff --hive-import -m 1
# 添加了--hive-overwrite参数是覆盖原有表格数据
bin/sqoop import --connect jdbc:mysql://主机IP:3306/company --username 用户名  --password 密码 --table staff --hive-table teststaff --hive-import -m 1 --hive-overwrite
# --where条件追加（注：where条件需要使用双引号括起来才能生效）
bin/sqoop import --connect jdbc:mysql://192.168.43.50:3306/company --username 用户名  --password 密码 --table staff --hive-table teststaff --where "id>2" --hive-import -m 1
# --query指定具体的查询语句，该参数不能喝--table一起使用(因为在sql中已经指明了具体的表名)
# 注意这里的查询语句需要加上and \$CONDITIONS; 还有需要使用--target-dir指定数据库表上传到hdfs上的路径
bin/sqoop import --connect jdbc:mysql://192.168.43.50:3306/company --username 用户名  --password 密码  --hive-table teststaff  --query "select id,name,sex from staff where id = 1 and \$CONDITIONS;" --hive-import -m 1 --target-dir /user/root/staff

#将数据库中的数据导入到hdfs
bin/sqoop import --connect jdbc:mysql://192.168.43.50:3306/company --username 用户名  --password 密码  --table staff --target-dir /company -m 1
```

```shell
# export命令是把hdfs或者hive上的数据导入到数据库
# export导出到数据库时，该数据库中的表必须已经存在
# export命令默认是导入以,为分割符的文件
# 但是hive的数据默认分隔符为'\u001'
bin/sqoop export --connect jdbc:mysql://192.168.43.50:3306/company --username 用户名  --password 密码  --table staff --export-dir /company

# 通过下面语句可以指定分隔符，还能设置更新方式，allowinsert是增量插入，配合update-key指定要匹配的列，如果id相同的话进行更新操作，不存在的话执行插入操作。
bin/sqoop export --connect jdbc:mysql://192.168.43.50:3306/company --username 用户名  --password 密码  --table staff --export-dir /user/hive/warehouse/teststaff --input-fields-terminated-by "\\01" --update-mode allowinsert --update-key id
```

